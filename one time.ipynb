{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: []\n",
      "Loading metadata from CSV...\n",
      "Loaded 463 entries from metadata CSV\n",
      "          case_id               image_id projection findings\n",
      "0  LIDC-IDRI-0001  LIDC-IDRI-0001-000001    Frontal  Nodules\n",
      "1  LIDC-IDRI-0001  LIDC-IDRI-0001-000002    Lateral  Nodules\n",
      "2  LIDC-IDRI-0003  LIDC-IDRI-0003-000001    Frontal  Nodules\n",
      "3  LIDC-IDRI-0003  LIDC-IDRI-0003-000002    Lateral  Nodules\n",
      "4  LIDC-IDRI-0004  LIDC-IDRI-0004-000001    Frontal  Nodules\n",
      "\n",
      "Exploring dataset structure:\n",
      "Found 11 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\157\n",
      "Sample files: ['158.xml', '159.xml', '160.xml']\n",
      "Found 232 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\185\n",
      "Sample files: ['068.xml', '069.xml', '070.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\186\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\187\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\188\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 175 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\189\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Starting Nodule Detection Pipeline\n",
      "\n",
      "Preparing dataset...\n",
      "Searching for XML files...\n",
      "Found 1319 XML files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing XML files: 100%|██████████| 1319/1319 [00:43<00:00, 30.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 0 XML files with 0 annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing from metadata: 100%|██████████| 463/463 [00:01<00:00, 287.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initially has 440 positive and 23 negative samples\n",
      "Balanced dataset to 69 positive and 23 negative samples\n",
      "Final dataset has 92 entries\n",
      "\n",
      "Preprocessing dataset...\n",
      "Split dataset into Train: 64, Validation: 14, Test: 14\n",
      "Building hybrid model...\n",
      "Building CNN branch...\n",
      "Building ViT branch...\n",
      "Building Vision Transformer...\n",
      "Building BiLSTM branch...\n",
      "Building segmentation branch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ lstm_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile_layer_53       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TileLayer</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vit_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_26    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ tile_layer_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_transformer  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">316,480</span> │ vit_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_513 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ bidirectional_26… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_499 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_512 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ vision_transform… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_284         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_513[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_499[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_512[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dropout_284[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_514 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ concatenate_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_514[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_285         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_515 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,223,808</span> │ dropout_285[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_81          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_515[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │ reshape_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_63       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │ conv2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_64       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │ conv2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_65       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,624</span> │ conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_66       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,160</span> │ conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ conv2d_transpose… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_67       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_285[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segmentation        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │ activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m224\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m112\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ lstm_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m112\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, │     \u001b[38;5;34m18,496\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, │        \u001b[38;5;34m256\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, │     \u001b[38;5;34m73,856\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, │        \u001b[38;5;34m512\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile_layer_53       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTileLayer\u001b[0m)         │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vit_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_26    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m98,816\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m4,049,571\u001b[0m │ tile_layer_53[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_transformer  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │    \u001b[38;5;34m316,480\u001b[0m │ vit_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_513 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ bidirectional_26… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_499 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m163,968\u001b[0m │ efficientnetb0[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_512 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m8,320\u001b[0m │ vision_transform… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_284         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_513[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_499[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_512[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dropout_284[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_514 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m98,560\u001b[0m │ concatenate_27[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_514[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_285         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_515 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)     │  \u001b[38;5;34m3,223,808\u001b[0m │ dropout_285[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_81          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dense_515[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │    \u001b[38;5;34m295,040\u001b[0m │ reshape_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_63       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_78 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ activation_63[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │     \u001b[38;5;34m73,792\u001b[0m │ conv2d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_64       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_79 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ activation_64[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │     \u001b[38;5;34m18,464\u001b[0m │ conv2d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_65       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_80 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_65[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │      \u001b[38;5;34m4,624\u001b[0m │ conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │         \u001b[38;5;34m64\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_66       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_81 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │      \u001b[38;5;34m2,320\u001b[0m │ activation_66[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_1… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │      \u001b[38;5;34m1,160\u001b[0m │ conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │         \u001b[38;5;34m32\u001b[0m │ conv2d_transpose… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_67       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_285[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segmentation        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m9\u001b[0m │ activation_67[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,661,045</span> (33.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,661,045\u001b[0m (33.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,450,366</span> (32.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,450,366\u001b[0m (32.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">210,679</span> (822.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m210,679\u001b[0m (822.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['cnn_input', 'vit_input', 'lstm_input']. Received: the structure of inputs={'cnn_input': '*', 'vit_input': '*', 'lstm_input': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mInput 0 of layer \"vision_transformer\" is incompatible with the layer: expected shape=(None, 224, 224, 1), found shape=(None, 3, 224, 224)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs={'cnn_input': 'tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)', 'vit_input': 'tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)', 'lstm_input': 'tf.Tensor(shape=(None, 3, 224, 224, 1), dtype=float32)'}\n  • training=True\n  • mask={'cnn_input': 'None', 'vit_input': 'None', 'lstm_input': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1035\u001b[0m\n\u001b[0;32m   1032\u001b[0m         visualize_prediction(sample_image_path, trained_model, img_size)\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1035\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 1017\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1014\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m history, trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m evaluate_model(trained_model, test_dataset)\n",
      "Cell \u001b[1;32mIn[38], line 842\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, val_dataset, epochs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    840\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 842\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mInput 0 of layer \"vision_transformer\" is incompatible with the layer: expected shape=(None, 224, 224, 1), found shape=(None, 3, 224, 224)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs={'cnn_input': 'tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)', 'vit_input': 'tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)', 'lstm_input': 'tf.Tensor(shape=(None, 3, 224, 224, 1), dtype=float32)'}\n  • training=True\n  • mask={'cnn_input': 'None', 'vit_input': 'None', 'lstm_input': 'None'}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pydicom\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Dataset paths\n",
    "BASE_DIR = \"./dataset\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images/images\")\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, \"annotations/annotations/tcia-lidc-xml\")\n",
    "CSV_FILE = os.path.join(BASE_DIR, \"lidc_metadata.csv\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata from CSV...\")\n",
    "try:\n",
    "    metadata_df = pd.read_csv(CSV_FILE)\n",
    "    print(f\"Loaded {len(metadata_df)} entries from metadata CSV\")\n",
    "    print(metadata_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    # Create a backup empty dataframe if loading fails\n",
    "    metadata_df = pd.DataFrame(columns=['case_id', 'image_id', 'projection', 'findings'])\n",
    "\n",
    "# Print dataset structure for debugging\n",
    "print(\"\\nExploring dataset structure:\")\n",
    "for subdir in ['157', '185', '186', '187', '188', '189']:\n",
    "    subdir_path = os.path.join(ANNOTATION_DIR, subdir)\n",
    "    if os.path.exists(subdir_path):\n",
    "        xml_files = [f for f in os.listdir(subdir_path) if f.endswith('.xml')]\n",
    "        print(f\"Found {len(xml_files)} XML files in {subdir_path}\")\n",
    "        if xml_files:\n",
    "            print(f\"Sample files: {xml_files[:3]}\")\n",
    "\n",
    "# Improved XML parsing function\n",
    "def parse_xml_annotations(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract namespace if present\n",
    "        namespace = ''\n",
    "        if root.tag.startswith('{'):\n",
    "            namespace = root.tag.split('}')[0] + '}'\n",
    "        \n",
    "        # Try different paths to find image ID\n",
    "        image_id = None\n",
    "        possible_paths = [\n",
    "            \".//{}ResponseHeader/{}SeriesInstanceUid\".format(namespace, namespace),\n",
    "            \"./{}ResponseHeader/{}SeriesInstanceUid\".format(namespace, namespace),\n",
    "            \".//SeriesInstanceUid\",\n",
    "            \".//ResponseHeader/SeriesInstanceUid\",\n",
    "            \".//studyInstanceUID\",\n",
    "            \".//seriesUID\"\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            try:\n",
    "                element = root.find(path)\n",
    "                if element is not None and element.text:\n",
    "                    image_id = element.text\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # If we still don't have an image ID, try to get it from the filename\n",
    "        if not image_id:\n",
    "            filename = os.path.basename(xml_path)\n",
    "            if filename.endswith('.xml'):\n",
    "                filename = filename[:-4]  # Remove .xml extension\n",
    "            image_id = filename\n",
    "        \n",
    "        nodules = []\n",
    "        \n",
    "        # Try different paths to find nodules\n",
    "        nodule_paths = [\n",
    "            \".//{}readingSession/{}unblindedReadNodule\".format(namespace, namespace),\n",
    "            \".//readingSession/unblindedReadNodule\",\n",
    "            \".//unblindedReadNodule\",\n",
    "            \".//nodule\"\n",
    "        ]\n",
    "        \n",
    "        for nodule_path in nodule_paths:\n",
    "            nodule_elements = root.findall(nodule_path)\n",
    "            if nodule_elements:\n",
    "                for nodule in nodule_elements:\n",
    "                    # Try different paths for ROI\n",
    "                    roi_paths = [\n",
    "                        \".//{}roi\".format(namespace),\n",
    "                        \"./{}roi\".format(namespace),\n",
    "                        \".//roi\",\n",
    "                        \"./roi\"\n",
    "                    ]\n",
    "                    \n",
    "                    coords = []\n",
    "                    for roi_path in roi_paths:\n",
    "                        roi = nodule.find(roi_path)\n",
    "                        if roi is not None:\n",
    "                            # Try different paths for edge maps\n",
    "                            edge_map_paths = [\n",
    "                                \".//{}edgeMap\".format(namespace),\n",
    "                                \"./{}edgeMap\".format(namespace),\n",
    "                                \".//edgeMap\",\n",
    "                                \"./edgeMap\"\n",
    "                            ]\n",
    "                            \n",
    "                            for edge_map_path in edge_map_paths:\n",
    "                                edge_maps = roi.findall(edge_map_path)\n",
    "                                if edge_maps:\n",
    "                                    for edge_map in edge_maps:\n",
    "                                        x = None\n",
    "                                        y = None\n",
    "                                        \n",
    "                                        x_elements = edge_map.findall(\".//xCoord\") or edge_map.findall(\"./xCoord\")\n",
    "                                        if x_elements and x_elements[0].text:\n",
    "                                            x = int(x_elements[0].text)\n",
    "                                            \n",
    "                                        y_elements = edge_map.findall(\".//yCoord\") or edge_map.findall(\"./yCoord\")\n",
    "                                        if y_elements and y_elements[0].text:\n",
    "                                            y = int(y_elements[0].text)\n",
    "                                            \n",
    "                                        if x is not None and y is not None:\n",
    "                                            coords.append((x, y))\n",
    "                    \n",
    "                    # Try to get characteristics\n",
    "                    subtlety = 3  # Default value\n",
    "                    malignancy = 3  # Default value\n",
    "                    \n",
    "                    subtlety_paths = [\n",
    "                        \".//{}characteristics/{}subtlety\".format(namespace, namespace),\n",
    "                        \".//characteristics/subtlety\",\n",
    "                        \".//subtlety\"\n",
    "                    ]\n",
    "                    \n",
    "                    for path in subtlety_paths:\n",
    "                        element = nodule.find(path)\n",
    "                        if element is not None and element.text:\n",
    "                            try:\n",
    "                                subtlety = int(element.text)\n",
    "                                break\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    malignancy_paths = [\n",
    "                        \".//{}characteristics/{}malignancy\".format(namespace, namespace),\n",
    "                        \".//characteristics/malignancy\",\n",
    "                        \".//malignancy\"\n",
    "                    ]\n",
    "                    \n",
    "                    for path in malignancy_paths:\n",
    "                        element = nodule.find(path)\n",
    "                        if element is not None and element.text:\n",
    "                            try:\n",
    "                                malignancy = int(element.text)\n",
    "                                break\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    if coords:\n",
    "                        nodules.append({\n",
    "                            \"coords\": coords,\n",
    "                            \"subtlety\": subtlety,\n",
    "                            \"malignancy\": malignancy\n",
    "                        })\n",
    "        \n",
    "        return image_id, nodules\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {xml_path}: {str(e)}\")\n",
    "        return None, []\n",
    "\n",
    "# Function to find all XML files\n",
    "def find_all_xml_files():\n",
    "    print(\"Searching for XML files...\")\n",
    "    xml_files = []\n",
    "    for root, dirs, files in os.walk(ANNOTATION_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                xml_files.append(os.path.join(root, file))\n",
    "    print(f\"Found {len(xml_files)} XML files\")\n",
    "    return xml_files\n",
    "\n",
    "# Function to find image files by pattern matching\n",
    "def find_image_files(image_id):\n",
    "    # Try different patterns to match images\n",
    "    patterns = [\n",
    "        f\"{IMAGE_DIR}/**/{image_id}*.dcm\",\n",
    "        f\"{IMAGE_DIR}/**/*{image_id}*.dcm\",\n",
    "        f\"{IMAGE_DIR}/**/{image_id.replace('-', '_')}*.dcm\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        image_paths = glob.glob(pattern, recursive=True)\n",
    "        if image_paths:\n",
    "            return image_paths[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create binary masks from nodule coordinates\n",
    "def create_nodule_mask(image_shape, nodules):\n",
    "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    for nodule in nodules:\n",
    "        coords = nodule[\"coords\"]\n",
    "        if len(coords) > 2:\n",
    "            pts = np.array(coords, np.int32)\n",
    "            pts = pts.reshape((-1, 1, 2))\n",
    "            cv2.fillPoly(mask, [pts], 255)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Improved function to load DICOM images\n",
    "def load_dicom_image(image_path):\n",
    "    try:\n",
    "        # Try to load as DICOM\n",
    "        ds = pydicom.dcmread(image_path)\n",
    "        img = ds.pixel_array\n",
    "        \n",
    "        # Normalize to 0-255\n",
    "        img = img - np.min(img)\n",
    "        if np.max(img) > 0:\n",
    "            img = img / np.max(img) * 255\n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        # Check if image needs to be inverted (black background)\n",
    "        if np.mean(img) > 127:\n",
    "            img = 255 - img\n",
    "            \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DICOM {image_path}: {e}\")\n",
    "        \n",
    "        # Try to load as regular image\n",
    "        try:\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                return img\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "\n",
    "# Function to generate dataset with fallback mechanisms\n",
    "def prepare_dataset(max_samples=None):\n",
    "    print(\"\\nPreparing dataset...\")\n",
    "    \n",
    "    # Find all XML files\n",
    "    xml_files = find_all_xml_files()\n",
    "    \n",
    "    # Parse XML annotations with progress bar\n",
    "    annotations = {}\n",
    "    successful_files = 0\n",
    "    for xml_file in tqdm(xml_files, desc=\"Parsing XML files\"):\n",
    "        image_id, nodules = parse_xml_annotations(xml_file)\n",
    "        if image_id and nodules:\n",
    "            annotations[image_id] = nodules\n",
    "            successful_files += 1\n",
    "    \n",
    "    print(f\"Successfully parsed {successful_files} XML files with {len(annotations)} annotations\")\n",
    "    \n",
    "    # If we have metadata CSV, use it to match images\n",
    "    dataset = []\n",
    "    \n",
    "    if not metadata_df.empty:\n",
    "        for _, row in tqdm(metadata_df.iterrows(), desc=\"Processing from metadata\", total=len(metadata_df)):\n",
    "            image_id = row['image_id']\n",
    "            case_id = row['case_id']\n",
    "            \n",
    "            # Find image path\n",
    "            image_path = find_image_files(image_id)\n",
    "            \n",
    "            if not image_path:\n",
    "                continue\n",
    "            \n",
    "            # Check if we have annotation for this image\n",
    "            nodules = []\n",
    "            has_nodules = False\n",
    "            \n",
    "            # Try exact match\n",
    "            if image_id in annotations:\n",
    "                nodules = annotations[image_id]\n",
    "                has_nodules = len(nodules) > 0\n",
    "            else:\n",
    "                # Try partial match\n",
    "                for anno_id in annotations:\n",
    "                    if anno_id in image_id or image_id in anno_id:\n",
    "                        nodules = annotations[anno_id]\n",
    "                        has_nodules = len(nodules) > 0\n",
    "                        break\n",
    "            \n",
    "            # If no annotation found but CSV says there are nodules\n",
    "            if not nodules and row.get('findings', '') == 'Nodules':\n",
    "                has_nodules = True\n",
    "            \n",
    "            # Create dataset entry\n",
    "            dataset.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"case_id\": case_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"nodules\": nodules,\n",
    "                \"has_nodules\": has_nodules,\n",
    "                \"projection\": row.get('projection', 'Unknown')\n",
    "            })\n",
    "            \n",
    "            if max_samples and len(dataset) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    # If we couldn't create dataset from metadata, try direct file-based approach\n",
    "    if not dataset:\n",
    "        print(\"Creating dataset from direct file scanning...\")\n",
    "        image_files = glob.glob(f\"{IMAGE_DIR}/**/*.dcm\", recursive=True)\n",
    "        \n",
    "        for image_path in tqdm(image_files, desc=\"Scanning image files\"):\n",
    "            image_id = os.path.basename(image_path).split('.')[0]\n",
    "            \n",
    "            # Try to find a matching annotation\n",
    "            nodules = []\n",
    "            has_nodules = False\n",
    "            \n",
    "            # Try exact match\n",
    "            if image_id in annotations:\n",
    "                nodules = annotations[image_id]\n",
    "                has_nodules = len(nodules) > 0\n",
    "            else:\n",
    "                # Try partial match\n",
    "                for anno_id in annotations:\n",
    "                    if anno_id in image_id or image_id in anno_id:\n",
    "                        nodules = annotations[anno_id]\n",
    "                        has_nodules = len(nodules) > 0\n",
    "                        break\n",
    "            \n",
    "            # Create dataset entry\n",
    "            dataset.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"case_id\": image_id.split('-')[0] if '-' in image_id else image_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"nodules\": nodules,\n",
    "                \"has_nodules\": has_nodules,\n",
    "                \"projection\": \"Unknown\"\n",
    "            })\n",
    "            \n",
    "            if max_samples and len(dataset) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    # If still no dataset, create synthetic dataset for model architecture testing\n",
    "    if not dataset:\n",
    "        print(\"WARNING: Creating synthetic dataset for model testing...\")\n",
    "        image_files = glob.glob(f\"{IMAGE_DIR}/**/*.dcm\", recursive=True)\n",
    "        \n",
    "        if not image_files:\n",
    "            image_files = glob.glob(f\"{IMAGE_DIR}/**/*.*\", recursive=True)\n",
    "        \n",
    "        if image_files:\n",
    "            for i, image_path in enumerate(image_files[:max_samples or 100]):\n",
    "                image_id = f\"synthetic_{i}\"\n",
    "                \n",
    "                # Create synthetic annotation\n",
    "                has_nodules = random.choice([True, False])\n",
    "                nodules = []\n",
    "                \n",
    "                if has_nodules:\n",
    "                    # Load image to get dimensions\n",
    "                    img = load_dicom_image(image_path)\n",
    "                    if img is not None:\n",
    "                        h, w = img.shape[:2]\n",
    "                        \n",
    "                        # Create random polygon\n",
    "                        num_points = random.randint(3, 8)\n",
    "                        center_x = random.randint(w//4, 3*w//4)\n",
    "                        center_y = random.randint(h//4, 3*h//4)\n",
    "                        radius = random.randint(10, min(w, h)//8)\n",
    "                        \n",
    "                        coords = []\n",
    "                        for j in range(num_points):\n",
    "                            angle = j * (2 * np.pi / num_points)\n",
    "                            x = int(center_x + radius * np.cos(angle))\n",
    "                            y = int(center_y + radius * np.sin(angle))\n",
    "                            coords.append((x, y))\n",
    "                        \n",
    "                        nodules.append({\n",
    "                            \"coords\": coords,\n",
    "                            \"subtlety\": random.randint(1, 5),\n",
    "                            \"malignancy\": random.randint(1, 5)\n",
    "                        })\n",
    "                \n",
    "                dataset.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"case_id\": f\"case_{i}\",\n",
    "                    \"image_path\": image_path,\n",
    "                    \"nodules\": nodules,\n",
    "                    \"has_nodules\": has_nodules,\n",
    "                    \"projection\": random.choice([\"Frontal\", \"Lateral\"])\n",
    "                })\n",
    "    \n",
    "    # Balance dataset if needed\n",
    "    if dataset:\n",
    "        # Count positives and negatives\n",
    "        positives = sum(1 for item in dataset if item[\"has_nodules\"])\n",
    "        negatives = len(dataset) - positives\n",
    "        \n",
    "        print(f\"Dataset initially has {positives} positive and {negatives} negative samples\")\n",
    "        \n",
    "        # Balance if too skewed\n",
    "        if positives > 0 and negatives > 0 and (positives / negatives > 3 or negatives / positives > 3):\n",
    "            if positives > negatives:\n",
    "                target_count = min(negatives * 3, positives)\n",
    "                positive_samples = random.sample([item for item in dataset if item[\"has_nodules\"]], target_count)\n",
    "                negative_samples = [item for item in dataset if not item[\"has_nodules\"]]\n",
    "                dataset = positive_samples + negative_samples\n",
    "            else:\n",
    "                target_count = min(positives * 3, negatives)\n",
    "                negative_samples = random.sample([item for item in dataset if not item[\"has_nodules\"]], target_count)\n",
    "                positive_samples = [item for item in dataset if item[\"has_nodules\"]]\n",
    "                dataset = positive_samples + negative_samples\n",
    "            \n",
    "            print(f\"Balanced dataset to {len([item for item in dataset if item['has_nodules']])} positive and {len([item for item in dataset if not item['has_nodules']])} negative samples\")\n",
    "    \n",
    "    print(f\"Final dataset has {len(dataset)} entries\")\n",
    "    return dataset\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "def preprocess_dataset(dataset, img_size=(224, 224), batch_size=32):\n",
    "    print(\"\\nPreprocessing dataset...\")\n",
    "    \n",
    "    if not dataset:\n",
    "        raise ValueError(\"Dataset is empty. Cannot preprocess.\")\n",
    "    \n",
    "    # Split dataset\n",
    "    train_data, temp_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Split dataset into Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    def generate_data(data_list, augment=False):\n",
    "        print(f\"Generating data from {len(data_list)} samples {'with' if augment else 'without'} augmentation...\")\n",
    "        \n",
    "        for item in data_list:\n",
    "            try:\n",
    "                image_path = item[\"image_path\"]\n",
    "                img = load_dicom_image(image_path)\n",
    "                \n",
    "                if img is None:\n",
    "                    print(f\"Warning: Could not load image {image_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Create mask for nodules\n",
    "                mask = create_nodule_mask(img.shape, item[\"nodules\"])\n",
    "                \n",
    "                # Resize image and mask\n",
    "                img_resized = cv2.resize(img, img_size)\n",
    "                mask_resized = cv2.resize(mask, img_size)\n",
    "                \n",
    "                # Data augmentation for training\n",
    "                if augment:\n",
    "                    # Random horizontal flip\n",
    "                    if random.random() > 0.5:\n",
    "                        img_resized = cv2.flip(img_resized, 1)\n",
    "                        mask_resized = cv2.flip(mask_resized, 1)\n",
    "                    \n",
    "                    # Random rotation\n",
    "                    if random.random() > 0.5:\n",
    "                        angle = random.uniform(-15, 15)\n",
    "                        M = cv2.getRotationMatrix2D((img_size[0]//2, img_size[1]//2), angle, 1.0)\n",
    "                        img_resized = cv2.warpAffine(img_resized, M, img_size)\n",
    "                        mask_resized = cv2.warpAffine(mask_resized, M, img_size)\n",
    "                    \n",
    "                    # Random brightness and contrast\n",
    "                    if random.random() > 0.5:\n",
    "                        alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "                        beta = random.uniform(-10, 10)    # Brightness\n",
    "                        img_resized = cv2.convertScaleAbs(img_resized, alpha=alpha, beta=beta)\n",
    "                \n",
    "                # Normalize the image\n",
    "                img_resized = img_resized / 255.0\n",
    "                \n",
    "                # Prepare binary mask\n",
    "                mask_resized = (mask_resized > 0).astype(np.float32)\n",
    "                \n",
    "                # Create sequential data for BiLSTM\n",
    "                # Here we simulate sequential data by creating multiple \"views\" of the same image\n",
    "                seq_data = []\n",
    "                for i in range(3):  # 3 frames for the sequence\n",
    "                    # Zoom in/out effect\n",
    "                    scale = 1.0 - (i * 0.1)\n",
    "                    width = int(img_size[0] * scale)\n",
    "                    height = int(img_size[1] * scale)\n",
    "                    \n",
    "                    if width < img_size[0] and height < img_size[1]:\n",
    "                        x = (img_size[0] - width) // 2\n",
    "                        y = (img_size[1] - height) // 2\n",
    "                        \n",
    "                        roi = img_resized[y:y+height, x:x+width]\n",
    "                        roi = cv2.resize(roi, img_size)\n",
    "                        seq_data.append(roi)\n",
    "                    else:\n",
    "                        seq_data.append(img_resized)\n",
    "                \n",
    "                seq_data = np.array(seq_data)\n",
    "                \n",
    "                # Use one-hot encoding for classification\n",
    "                is_nodule = 1.0 if item[\"has_nodules\"] else 0.0\n",
    "                \n",
    "                # Create sample\n",
    "                sample = (\n",
    "                    {\n",
    "                        \"cnn_input\": np.expand_dims(np.expand_dims(img_resized, axis=-1), axis=0),\n",
    "                        \"vit_input\": np.expand_dims(np.expand_dims(img_resized, axis=-1), axis=0),\n",
    "                        \"lstm_input\": np.expand_dims(np.expand_dims(seq_data, axis=-1), axis=0)\n",
    "                    }, \n",
    "                    {\n",
    "                        \"classification\": np.array([is_nodule]),\n",
    "                        \"segmentation\": np.expand_dims(np.expand_dims(mask_resized, axis=0), axis=-1)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                yield sample\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {item.get('image_path', 'unknown')}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def create_tf_dataset(data_list, augment=False):\n",
    "        output_signature = (\n",
    "            {\n",
    "                \"cnn_input\": tf.TensorSpec(shape=(1, img_size[0], img_size[1], 1), dtype=tf.float32),\n",
    "                \"vit_input\": tf.TensorSpec(shape=(1, img_size[0], img_size[1], 1), dtype=tf.float32),\n",
    "                \"lstm_input\": tf.TensorSpec(shape=(1, 3, img_size[0], img_size[1], 1), dtype=tf.float32)\n",
    "            },\n",
    "            {\n",
    "                \"classification\": tf.TensorSpec(shape=(1,), dtype=tf.float32),\n",
    "                \"segmentation\": tf.TensorSpec(shape=(1, img_size[0], img_size[1], 1), dtype=tf.float32)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create the dataset\n",
    "        try:\n",
    "            dataset = tf.data.Dataset.from_generator(\n",
    "                lambda: generate_data(data_list, augment),\n",
    "                output_signature=output_signature\n",
    "            )\n",
    "            \n",
    "            # Unbatch the data (remove the batch dimension of 1)\n",
    "            dataset = dataset.map(lambda x, y: (\n",
    "                {\n",
    "                    \"cnn_input\": x[\"cnn_input\"][0],\n",
    "                    \"vit_input\": x[\"vit_input\"][0],\n",
    "                    \"lstm_input\": x[\"lstm_input\"][0]\n",
    "                },\n",
    "                {\n",
    "                    \"classification\": y[\"classification\"][0],\n",
    "                    \"segmentation\": y[\"segmentation\"][0]\n",
    "                }\n",
    "            ))\n",
    "            \n",
    "            # Recache for faster access\n",
    "            dataset = dataset.cache()\n",
    "            \n",
    "            # Batch the data\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating TensorFlow dataset: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = create_tf_dataset(train_data, augment=True)\n",
    "    val_dataset = create_tf_dataset(val_data, augment=False)\n",
    "    test_dataset = create_tf_dataset(test_data, augment=False)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ...existing code...\n",
    "# Define Vision Transformer Block\n",
    "def vision_transformer_block(inputs, dim, num_heads):\n",
    "    # Layer Normalization 1\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=dim // num_heads\n",
    "    )(x, x)\n",
    "    \n",
    "    # Skip Connection 1\n",
    "    x = layers.Add()([attention_output, inputs])\n",
    "    \n",
    "    # Layer Normalization 2\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # MLP\n",
    "    y = layers.Dense(dim * 4, activation=\"gelu\")(y)\n",
    "    y = layers.Dense(dim)(y)\n",
    "    y = layers.Dropout(0.1)(y)\n",
    "    \n",
    "    # Skip Connection 2\n",
    "    out = layers.Add()([y, x])\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Build the Vision Transformer Module\n",
    "\n",
    "def build_vit(img_size=(224, 224), patch_size=16, num_heads=8, transformer_layers=6):\n",
    "    print(\"Building Vision Transformer...\")\n",
    "    \n",
    "    inputs = layers.Input(shape=(img_size[0], img_size[1], 1))\n",
    "    \n",
    "    # Patch embedding\n",
    "    x = layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding=\"valid\"\n",
    "    )(inputs)\n",
    "    \n",
    "    # Calculate dimensions\n",
    "    h = img_size[0] // patch_size\n",
    "    w = img_size[1] // patch_size\n",
    "    \n",
    "    # Reshape patches\n",
    "    x = layers.Reshape((h * w, 64))(x)\n",
    "    \n",
    "    # Add position embedding\n",
    "    positions = tf.range(start=0, limit=h * w, delta=1)\n",
    "    pos_embedding = layers.Embedding(input_dim=h * w, output_dim=64)(positions)\n",
    "    x = x + pos_embedding\n",
    "    \n",
    "    # Transformer layers\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer Normalization 1\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Multi-Head Self Attention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=64 // num_heads,\n",
    "            value_dim=64 // num_heads\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip Connection 1\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        \n",
    "        # Layer Normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # MLP\n",
    "        x3 = layers.Dense(64 * 4, activation=\"gelu\")(x3)\n",
    "        x3 = layers.Dense(64)(x3)\n",
    "        x3 = layers.Dropout(0.1)(x3)\n",
    "        \n",
    "        # Skip Connection 2\n",
    "        x = layers.Add()([x2, x3])\n",
    "    \n",
    "    # Final Layer Norm\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=\"vision_transformer\")\n",
    "\n",
    "class ExpandDimsLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, -1)\n",
    "\n",
    "class TileLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.tile(inputs, [1, 1, 1, 3])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], 3)\n",
    "\n",
    "\n",
    "\n",
    "# Build the Hybrid Model\n",
    "\n",
    "def build_hybrid_model(img_size=(224, 224)):\n",
    "    print(\"Building hybrid model...\")\n",
    "    \n",
    "    # 1. Input layers\n",
    "    cnn_input = layers.Input(shape=(img_size[0], img_size[1], 1), name=\"cnn_input\")\n",
    "    vit_input = layers.Input(shape=(img_size[0], img_size[1], 1), name=\"vit_input\")\n",
    "    lstm_input = layers.Input(shape=(3, img_size[0], img_size[1], 1), name=\"lstm_input\")\n",
    "    \n",
    "    # 2. CNN Branch\n",
    "    print(\"Building CNN branch...\")\n",
    "    x_cnn = TileLayer()(cnn_input)\n",
    "    efficient_net = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_size[0], img_size[1], 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    for layer in efficient_net.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    x_cnn = efficient_net(x_cnn)\n",
    "    x_cnn = layers.Dense(128, activation='relu')(x_cnn)\n",
    "    \n",
    "    # 3. ViT Branch (in build_hybrid_model function)\n",
    "    print(\"Building ViT branch...\")\n",
    "    vit_model = build_vit(img_size=img_size)\n",
    "    x_vit = vit_model(vit_input)\n",
    "    x_vit = layers.Dense(128, activation='relu')(x_vit)\n",
    "    \n",
    "    # 4. BiLSTM Branch\n",
    "    print(\"Building BiLSTM branch...\")\n",
    "    x_lstm = layers.TimeDistributed(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu'))(lstm_input)\n",
    "    x_lstm = layers.TimeDistributed(layers.BatchNormalization())(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.TimeDistributed(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))(x_lstm)\n",
    "    x_lstm = layers.TimeDistributed(layers.BatchNormalization())(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.TimeDistributed(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))(x_lstm)\n",
    "    x_lstm = layers.TimeDistributed(layers.BatchNormalization())(x_lstm)\n",
    "    \n",
    "    # Global average pooling instead of flatten\n",
    "    x_lstm = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x_lstm)\n",
    "    \n",
    "    # BiLSTM layer\n",
    "    x_lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x_lstm)\n",
    "    x_lstm = layers.Dense(128, activation='relu')(x_lstm)\n",
    "    x_lstm = layers.Dropout(0.3)(x_lstm)\n",
    "\n",
    "    \n",
    "    # 5. Feature Fusion\n",
    "    combined_features = layers.concatenate([x_cnn, x_vit, x_lstm])\n",
    "    shared_features = layers.Dense(256, activation='relu')(combined_features)\n",
    "    shared_features = layers.BatchNormalization()(shared_features)\n",
    "    shared_features = layers.Dropout(0.3)(shared_features)\n",
    "    \n",
    "    # 6. Classification Branch\n",
    "    classification_output = layers.Dense(1, activation='sigmoid', name=\"classification\")(shared_features)\n",
    "    \n",
    "    # 7. Segmentation Branch\n",
    "    print(\"Building segmentation branch...\")\n",
    "    initial_size = img_size[0] // 32\n",
    "    initial_channels = 256\n",
    "    \n",
    "    x_seg = layers.Dense(initial_size * initial_size * initial_channels)(shared_features)\n",
    "    x_seg = layers.Reshape((initial_size, initial_size, initial_channels))(x_seg)\n",
    "    \n",
    "    # Upsampling blocks\n",
    "    for filters in [128, 64, 32, 16]:\n",
    "        x_seg = layers.Conv2DTranspose(filters, 3, strides=2, padding='same')(x_seg)\n",
    "        x_seg = layers.BatchNormalization()(x_seg)\n",
    "        x_seg = layers.Activation('relu')(x_seg)\n",
    "        x_seg = layers.Conv2D(filters, 3, padding='same', activation='relu')(x_seg)\n",
    "    \n",
    "    # Final upsampling and convolution\n",
    "    x_seg = layers.Conv2DTranspose(8, 3, strides=2, padding='same')(x_seg)\n",
    "    x_seg = layers.BatchNormalization()(x_seg)\n",
    "    x_seg = layers.Activation('relu')(x_seg)\n",
    "    segmentation_output = layers.Conv2D(1, 1, activation='sigmoid', name=\"segmentation\")(x_seg)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(\n",
    "        inputs=[cnn_input, vit_input, lstm_input],\n",
    "        outputs=[classification_output, segmentation_output]\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            \"classification\": \"binary_crossentropy\",\n",
    "            \"segmentation\": \"binary_crossentropy\"\n",
    "        },\n",
    "        metrics={\n",
    "            \"classification\": [\"accuracy\", tf.keras.metrics.AUC()],\n",
    "            \"segmentation\": [\"accuracy\", tf.keras.metrics.IoU(num_classes=2, target_class_ids=[1])]\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"classification\": 1.0,\n",
    "            \"segmentation\": 0.5\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=5):\n",
    "    # Create callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"best_model.keras\",\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        update_freq=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, test_dataset):\n",
    "    print(\"Evaluating model...\")\n",
    "    results = model.evaluate(test_dataset, verbose=1)\n",
    "    \n",
    "    metrics = model.metrics_names\n",
    "    for i, metric in enumerate(metrics):\n",
    "        print(f\"{metric}: {results[i]:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot classification accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['classification_accuracy'])\n",
    "    plt.plot(history.history['val_classification_accuracy'])\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot classification loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['classification_loss'])\n",
    "    plt.plot(history.history['val_classification_loss'])\n",
    "    plt.title('Classification Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot segmentation accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['segmentation_accuracy'])\n",
    "    plt.plot(history.history['val_segmentation_accuracy'])\n",
    "    plt.title('Segmentation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot segmentation loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['segmentation_loss'])\n",
    "    plt.plot(history.history['val_segmentation_loss'])\n",
    "    plt.title('Segmentation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "# Prediction function\n",
    "def predict_nodules(model, image_path, img_size=(224, 224)):\n",
    "    # Load and preprocess image\n",
    "    img = load_dicom_image(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Resize and normalize\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    img_normalized = img_resized / 255.0\n",
    "    \n",
    "    # Create sequential data\n",
    "    seq_data = []\n",
    "    for i in range(3):\n",
    "        scale = 1.0 - (i * 0.1)\n",
    "        width = int(img_size[0] * scale)\n",
    "        height = int(img_size[1] * scale)\n",
    "        \n",
    "        if width < img_size[0] and height < img_size[1]:\n",
    "            x = (img_size[0] - width) // 2\n",
    "            y = (img_size[1] - height) // 2\n",
    "            \n",
    "            roi = img_normalized[y:y+height, x:x+width]\n",
    "            roi = cv2.resize(roi, img_size)\n",
    "            seq_data.append(roi)\n",
    "        else:\n",
    "            seq_data.append(img_normalized)\n",
    "    \n",
    "    seq_data = np.array(seq_data)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = {\n",
    "        \"cnn_input\": np.expand_dims(img_normalized, axis=0),\n",
    "        \"vit_input\": np.expand_dims(img_normalized, axis=0),\n",
    "        \"lstm_input\": np.expand_dims(seq_data, axis=0)\n",
    "    }\n",
    "    \n",
    "    # Make prediction\n",
    "    nodule_prob, nodule_mask = model.predict(inputs)\n",
    "    \n",
    "    # Post-process mask\n",
    "    nodule_mask = nodule_mask[0]\n",
    "    nodule_mask = (nodule_mask > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    return nodule_prob[0][0], nodule_mask\n",
    "\n",
    "# Visualize predictions\n",
    "def visualize_prediction(image_path, model, img_size=(224, 224)):\n",
    "    # Load image\n",
    "    img = load_dicom_image(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Resize image\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Make prediction\n",
    "    prob, mask = predict_nodules(model, image_path, img_size)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.title(f'Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.7)\n",
    "    plt.title(f'Predicted Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.4)\n",
    "    plt.title(f'Nodule Prob: {prob:.2f}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_visualization.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting Nodule Detection Pipeline\")\n",
    "    \n",
    "    # Prepare dataset (limit samples for faster testing)\n",
    "    # For full training, remove the max_samples parameter\n",
    "    dataset = prepare_dataset(max_samples=1000)\n",
    "    \n",
    "    # Preprocess dataset\n",
    "    img_size = (224, 224)\n",
    "    batch_size = 8\n",
    "    train_dataset, val_dataset, test_dataset = preprocess_dataset(dataset, img_size, batch_size)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_hybrid_model(img_size)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history, trained_model = train_model(model, train_dataset, val_dataset, epochs=5)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(trained_model, test_dataset)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save model\n",
    "    trained_model.save('nodule_detection_model.h5')\n",
    "    print(\"Model saved as 'nodule_detection_model.h5'\")\n",
    "    \n",
    "    # Test prediction on a sample image\n",
    "    if dataset:\n",
    "        sample_image_path = dataset[0][\"image_path\"]\n",
    "        visualize_prediction(sample_image_path, trained_model, img_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: []\n",
      "Loading metadata from CSV...\n",
      "Loaded 463 entries from metadata CSV\n",
      "          case_id               image_id projection findings\n",
      "0  LIDC-IDRI-0001  LIDC-IDRI-0001-000001    Frontal  Nodules\n",
      "1  LIDC-IDRI-0001  LIDC-IDRI-0001-000002    Lateral  Nodules\n",
      "2  LIDC-IDRI-0003  LIDC-IDRI-0003-000001    Frontal  Nodules\n",
      "3  LIDC-IDRI-0003  LIDC-IDRI-0003-000002    Lateral  Nodules\n",
      "4  LIDC-IDRI-0004  LIDC-IDRI-0004-000001    Frontal  Nodules\n",
      "\n",
      "Exploring dataset structure:\n",
      "Found 11 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\157\n",
      "Sample files: ['158.xml', '159.xml', '160.xml']\n",
      "Found 232 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\185\n",
      "Sample files: ['068.xml', '069.xml', '070.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\186\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\187\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 300 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\188\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Found 175 XML files in ./dataset\\annotations/annotations/tcia-lidc-xml\\189\n",
      "Sample files: ['000.xml', '001.xml', '002.xml']\n",
      "Starting Nodule Detection Pipeline\n",
      "Checking GPU availability...\n",
      "No GPU found, using CPU\n",
      "Loading metadata from ./dataset\\lidc_metadata.csv\n",
      "\n",
      "Preparing dataset...\n",
      "Searching for XML files...\n",
      "Found 1319 XML files\n",
      "Found 1319 XML files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing XML files: 100%|██████████| 1319/1319 [00:41<00:00, 31.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing Summary:\n",
      "Successfully parsed 0 XML files\n",
      "Total nodules found: 0\n",
      "Total unique images with annotations: 0\n",
      "\n",
      "Processing from metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing metadata entries: 463it [00:42, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning image files directly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing image files: 100%|██████████| 463/463 [00:00<00:00, 48746.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset balance:\n",
      "Positive samples (with nodules): 440\n",
      "Negative samples (without nodules): 23\n",
      "After balancing:\n",
      "Positive samples: 69\n",
      "Negative samples: 23\n",
      "\n",
      "Final dataset has 92 entries\n",
      "\n",
      "Using batch size: 4, image size: (224, 224)\n",
      "\n",
      "Preprocessing dataset...\n",
      "\n",
      "Building model...\n",
      "Building EfficientNet model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ tile_layer_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TileLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_25     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_426 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_686 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_180         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_427 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast_25 (\u001b[38;5;33mCast\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ tile_layer_74 (\u001b[38;5;33mTileLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_25     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_426 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_686 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_180         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_427 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,378,788</span> (16.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,378,788\u001b[0m (16.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,169,053</span> (15.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,169,053\u001b[0m (15.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,735</span> (819.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m209,735\u001b[0m (819.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1/10\n",
      "   2045/Unknown \u001b[1m6584s\u001b[0m 3s/step - accuracy: 0.6631 - auc_35: 0.7150 - loss: 0.6666"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pydicom\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Dataset paths\n",
    "BASE_DIR = \"./dataset\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images/images\")\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, \"annotations/annotations/tcia-lidc-xml\")\n",
    "CSV_FILE = os.path.join(BASE_DIR, \"lidc_metadata.csv\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata from CSV...\")\n",
    "try:\n",
    "    metadata_df = pd.read_csv(CSV_FILE)\n",
    "    print(f\"Loaded {len(metadata_df)} entries from metadata CSV\")\n",
    "    print(metadata_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    # Create a backup empty dataframe if loading fails\n",
    "    metadata_df = pd.DataFrame(columns=['case_id', 'image_id', 'projection', 'findings'])\n",
    "\n",
    "# Print dataset structure for debugging\n",
    "print(\"\\nExploring dataset structure:\")\n",
    "for subdir in ['157', '185', '186', '187', '188', '189']:\n",
    "    subdir_path = os.path.join(ANNOTATION_DIR, subdir)\n",
    "    if os.path.exists(subdir_path):\n",
    "        xml_files = [f for f in os.listdir(subdir_path) if f.endswith('.xml')]\n",
    "        print(f\"Found {len(xml_files)} XML files in {subdir_path}\")\n",
    "        if xml_files:\n",
    "            print(f\"Sample files: {xml_files[:3]}\")\n",
    "\n",
    "# Improved XML parsing function\n",
    "def parse_xml_annotations(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get the namespace from root tag\n",
    "        namespace = ''\n",
    "        if '}' in root.tag:\n",
    "            namespace = '{' + root.tag.split('}')[0].split('{')[1] + '}'\n",
    "        \n",
    "        # Get reading session elements\n",
    "        reading_sessions = root.findall(f\".//{namespace}readingSession\")\n",
    "        if not reading_sessions:\n",
    "            reading_sessions = root.findall(\".//readingSession\")\n",
    "        \n",
    "        # Get series UID\n",
    "        series_uid_elem = root.find(f\".//{namespace}SeriesInstanceUid\") or \\\n",
    "                         root.find(\".//SeriesInstanceUid\") or \\\n",
    "                         root.find(f\".//{namespace}seriesuid\") or \\\n",
    "                         root.find(\".//seriesuid\")\n",
    "        \n",
    "        image_id = series_uid_elem.text if series_uid_elem is not None else os.path.splitext(os.path.basename(xml_path))[0]\n",
    "        \n",
    "        nodules = []\n",
    "        for session in reading_sessions:\n",
    "            # Find nodule elements\n",
    "            nodule_elems = session.findall(f\".//{namespace}unblindedReadNodule\") or \\\n",
    "                          session.findall(\".//unblindedReadNodule\")\n",
    "            \n",
    "            for nodule in nodule_elems:\n",
    "                roi_elem = nodule.find(f\".//{namespace}roi\") or nodule.find(\".//roi\")\n",
    "                if roi_elem is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get coordinates\n",
    "                coords = []\n",
    "                edge_maps = roi_elem.findall(f\".//{namespace}edgeMap\") or roi_elem.findall(\".//edgeMap\")\n",
    "                for edge_map in edge_maps:\n",
    "                    x_elem = edge_map.find(f\".//{namespace}xCoord\") or edge_map.find(\".//xCoord\")\n",
    "                    y_elem = edge_map.find(f\".//{namespace}yCoord\") or edge_map.find(\".//yCoord\")\n",
    "                    \n",
    "                    if x_elem is not None and y_elem is not None:\n",
    "                        try:\n",
    "                            x = int(float(x_elem.text))\n",
    "                            y = int(float(y_elem.text))\n",
    "                            coords.append((x, y))\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue\n",
    "                \n",
    "                if len(coords) < 3:  # Need at least 3 points for a polygon\n",
    "                    continue\n",
    "                \n",
    "                # Get characteristics\n",
    "                chars_elem = nodule.find(f\".//{namespace}characteristics\") or nodule.find(\".//characteristics\")\n",
    "                subtlety = malignancy = 3  # Default values\n",
    "                \n",
    "                if chars_elem is not None:\n",
    "                    subt_elem = chars_elem.find(f\".//{namespace}subtlety\") or chars_elem.find(\".//subtlety\")\n",
    "                    malig_elem = chars_elem.find(f\".//{namespace}malignancy\") or chars_elem.find(\".//malignancy\")\n",
    "                    \n",
    "                    if subt_elem is not None and subt_elem.text:\n",
    "                        subtlety = int(float(subt_elem.text))\n",
    "                    if malig_elem is not None and malig_elem.text:\n",
    "                        malignancy = int(float(malig_elem.text))\n",
    "                \n",
    "                nodules.append({\n",
    "                    \"coords\": coords,\n",
    "                    \"subtlety\": subtlety,\n",
    "                    \"malignancy\": malignancy\n",
    "                })\n",
    "        \n",
    "        if nodules:\n",
    "            print(f\"Successfully parsed {len(nodules)} nodules from {os.path.basename(xml_path)}\")\n",
    "            return image_id, nodules\n",
    "        \n",
    "        return None, []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {xml_path}: {str(e)}\")\n",
    "        return None, []\n",
    "\n",
    "# Function to find all XML files\n",
    "def find_all_xml_files():\n",
    "    print(\"Searching for XML files...\")\n",
    "    xml_files = []\n",
    "    for root, dirs, files in os.walk(ANNOTATION_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                xml_files.append(os.path.join(root, file))\n",
    "    print(f\"Found {len(xml_files)} XML files\")\n",
    "    return xml_files\n",
    "\n",
    "# Function to find image files by pattern matching\n",
    "def find_image_files(image_id):\n",
    "    # Try different patterns to match images\n",
    "    patterns = [\n",
    "        f\"{IMAGE_DIR}/**/{image_id}*.dcm\",\n",
    "        f\"{IMAGE_DIR}/**/*{image_id}*.dcm\",\n",
    "        f\"{IMAGE_DIR}/**/{image_id.replace('-', '_')}*.dcm\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        image_paths = glob.glob(pattern, recursive=True)\n",
    "        if image_paths:\n",
    "            return image_paths[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create binary masks from nodule coordinates\n",
    "def create_nodule_mask(image_shape, nodules):\n",
    "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    for nodule in nodules:\n",
    "        coords = nodule[\"coords\"]\n",
    "        if len(coords) > 2:\n",
    "            pts = np.array(coords, np.int32)\n",
    "            pts = pts.reshape((-1, 1, 2))\n",
    "            cv2.fillPoly(mask, [pts], 255)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Improved function to load DICOM images\n",
    "def load_dicom_image(image_path):\n",
    "    try:\n",
    "        # Try to load as DICOM\n",
    "        ds = pydicom.dcmread(image_path)\n",
    "        img = ds.pixel_array\n",
    "        \n",
    "        # Normalize to 0-255\n",
    "        img = img - np.min(img)\n",
    "        if np.max(img) > 0:\n",
    "            img = img / np.max(img) * 255\n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        # Check if image needs to be inverted (black background)\n",
    "        if np.mean(img) > 127:\n",
    "            img = 255 - img\n",
    "            \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DICOM {image_path}: {e}\")\n",
    "        \n",
    "        # Try to load as regular image\n",
    "        try:\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                return img\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "\n",
    "# Function to generate dataset with fallback mechanisms\n",
    "def prepare_dataset(max_samples=None):\n",
    "    print(\"\\nPreparing dataset...\")\n",
    "    \n",
    "    # Validate directories\n",
    "    if not os.path.exists(IMAGE_DIR):\n",
    "        raise ValueError(f\"Image directory not found: {IMAGE_DIR}\")\n",
    "    if not os.path.exists(ANNOTATION_DIR):\n",
    "        raise ValueError(f\"Annotation directory not found: {ANNOTATION_DIR}\")\n",
    "    \n",
    "    # Find all XML files\n",
    "    xml_files = find_all_xml_files()\n",
    "    if not xml_files:\n",
    "        raise ValueError(\"No XML files found in the annotation directory\")\n",
    "    \n",
    "    print(f\"Found {len(xml_files)} XML files\")\n",
    "    \n",
    "    # Parse XML annotations with progress bar\n",
    "    annotations = {}\n",
    "    successful_files = 0\n",
    "    total_nodules = 0\n",
    "    \n",
    "    for xml_file in tqdm(xml_files, desc=\"Parsing XML files\"):\n",
    "        image_id, nodules = parse_xml_annotations(xml_file)\n",
    "        if image_id and nodules:\n",
    "            annotations[image_id] = nodules\n",
    "            successful_files += 1\n",
    "            total_nodules += len(nodules)\n",
    "    \n",
    "    print(f\"\\nParsing Summary:\")\n",
    "    print(f\"Successfully parsed {successful_files} XML files\")\n",
    "    print(f\"Total nodules found: {total_nodules}\")\n",
    "    print(f\"Total unique images with annotations: {len(annotations)}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = []\n",
    "    \n",
    "    # Try metadata-based approach first\n",
    "    if not metadata_df.empty:\n",
    "        print(\"\\nProcessing from metadata...\")\n",
    "        for _, row in tqdm(metadata_df.iterrows(), desc=\"Processing metadata entries\"):\n",
    "            image_id = str(row['image_id'])\n",
    "            case_id = str(row['case_id'])\n",
    "            \n",
    "            # Find image path\n",
    "            image_path = find_image_files(image_id)\n",
    "            if not image_path:\n",
    "                continue\n",
    "            \n",
    "            # Verify image can be loaded\n",
    "            img = load_dicom_image(image_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Get nodule information\n",
    "            nodules = annotations.get(image_id, [])\n",
    "            has_nodules = bool(nodules) or ('findings' in row and 'Nodules' in str(row['findings']))\n",
    "            \n",
    "            dataset.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"case_id\": case_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"nodules\": nodules,\n",
    "                \"has_nodules\": has_nodules,\n",
    "                \"projection\": row.get('projection', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # If dataset is too small, try direct file scanning\n",
    "    if len(dataset) < (max_samples or 100):\n",
    "        print(\"\\nScanning image files directly...\")\n",
    "        image_files = glob.glob(f\"{IMAGE_DIR}/**/*.dcm\", recursive=True)\n",
    "        \n",
    "        for image_path in tqdm(image_files, desc=\"Processing image files\"):\n",
    "            if any(d['image_path'] == image_path for d in dataset):\n",
    "                continue\n",
    "                \n",
    "            image_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            \n",
    "            # Verify image can be loaded\n",
    "            img = load_dicom_image(image_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            nodules = annotations.get(image_id, [])\n",
    "            dataset.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"case_id\": image_id.split('-')[0],\n",
    "                \"image_path\": image_path,\n",
    "                \"nodules\": nodules,\n",
    "                \"has_nodules\": bool(nodules),\n",
    "                \"projection\": \"Unknown\"\n",
    "            })\n",
    "    \n",
    "    # Balance dataset\n",
    "    if dataset:\n",
    "        positives = sum(1 for item in dataset if item[\"has_nodules\"])\n",
    "        negatives = len(dataset) - positives\n",
    "        \n",
    "        print(f\"\\nDataset balance:\")\n",
    "        print(f\"Positive samples (with nodules): {positives}\")\n",
    "        print(f\"Negative samples (without nodules): {negatives}\")\n",
    "        \n",
    "        # Balance if needed\n",
    "        if positives > 0 and negatives > 0:\n",
    "            if positives > negatives * 3:\n",
    "                target_count = negatives * 3\n",
    "                positive_samples = random.sample([item for item in dataset if item[\"has_nodules\"]], target_count)\n",
    "                negative_samples = [item for item in dataset if not item[\"has_nodules\"]]\n",
    "                dataset = positive_samples + negative_samples\n",
    "            elif negatives > positives * 3:\n",
    "                target_count = positives * 3\n",
    "                negative_samples = random.sample([item for item in dataset if not item[\"has_nodules\"]], target_count)\n",
    "                positive_samples = [item for item in dataset if item[\"has_nodules\"]]\n",
    "                dataset = positive_samples + negative_samples\n",
    "            \n",
    "            print(f\"After balancing:\")\n",
    "            print(f\"Positive samples: {len([item for item in dataset if item['has_nodules']])}\")\n",
    "            print(f\"Negative samples: {len([item for item in dataset if not item['has_nodules']])}\")\n",
    "    \n",
    "    if not dataset:\n",
    "        raise ValueError(\"Failed to create dataset. No valid images found.\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset has {len(dataset)} entries\")\n",
    "    return dataset\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "def preprocess_dataset(dataset, img_size=(224, 224), batch_size=32):\n",
    "    print(\"\\nPreprocessing dataset...\")\n",
    "    \n",
    "    def generate_data(data_list, augment=False):\n",
    "        while True:\n",
    "            for item in data_list:\n",
    "                try:\n",
    "                    image_path = item[\"image_path\"]\n",
    "                    img = load_dicom_image(image_path)\n",
    "                    \n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure consistent image shape\n",
    "                    img_resized = cv2.resize(img, img_size)\n",
    "                    img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "                    img_normalized = np.expand_dims(img_normalized, axis=-1)\n",
    "                    \n",
    "                    yield img_normalized, float(item[\"has_nodules\"])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Create datasets with correct shapes\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(img_size[0], img_size[1], 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    # Split dataset\n",
    "    train_data, temp_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Create tf.data.Dataset objects\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generate_data(train_data, augment=True),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generate_data(val_data, augment=False),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generate_data(test_data, augment=False),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ...existing code...\n",
    "# Define Vision Transformer Block\n",
    "def vision_transformer_block(inputs, dim, num_heads):\n",
    "    # Layer Normalization 1\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=dim // num_heads\n",
    "    )(x, x)\n",
    "    \n",
    "    # Skip Connection 1\n",
    "    x = layers.Add()([attention_output, inputs])\n",
    "    \n",
    "    # Layer Normalization 2\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # MLP\n",
    "    y = layers.Dense(dim * 4, activation=\"gelu\")(y)\n",
    "    y = layers.Dense(dim)(y)\n",
    "    y = layers.Dropout(0.1)(y)\n",
    "    \n",
    "    # Skip Connection 2\n",
    "    out = layers.Add()([y, x])\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Build the Vision Transformer Module\n",
    "\n",
    "def build_vit(img_size=(224, 224), patch_size=16, num_heads=8, transformer_layers=6):\n",
    "    print(\"Building Vision Transformer...\")\n",
    "    \n",
    "    # Input shape is (batch_size, height, width, channels)\n",
    "    inputs = layers.Input(shape=(img_size[0], img_size[1], 1))\n",
    "    \n",
    "    # Ensure correct shape before processing\n",
    "    x = layers.Permute((2, 3, 1))(inputs) if len(inputs.shape) == 4 and inputs.shape[1] == 3 else inputs\n",
    "    \n",
    "    # Convert to 3 channels using TileLayer\n",
    "    x = TileLayer()(x)\n",
    "    \n",
    "    # Patch embedding\n",
    "    patches = layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding=\"same\",\n",
    "        name=\"patch_embedding\"\n",
    "    )(x)\n",
    "    \n",
    "    # Calculate patch dimensions\n",
    "    patch_h = img_size[0] // patch_size\n",
    "    patch_w = img_size[1] // patch_size\n",
    "    num_patches = patch_h * patch_w\n",
    "    \n",
    "    # Reshape patches\n",
    "    x = layers.Reshape((num_patches, 64))(patches)\n",
    "    \n",
    "    # Add position embedding\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    pos_embed = layers.Embedding(input_dim=num_patches, output_dim=64)(positions)\n",
    "    x = x + pos_embed\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for i in range(transformer_layers):\n",
    "        x = vision_transformer_block(x, dim=64, num_heads=num_heads)\n",
    "    \n",
    "    # Final processing\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=\"vision_transformer\")\n",
    "\n",
    "class ExpandDimsLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, -1)\n",
    "\n",
    "class TileLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # Ensure input is float32\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if len(x.shape) == 4:  # (batch, height, width, channels)\n",
    "            if x.shape[-1] == 1:\n",
    "                # Tile the channels dimension\n",
    "                return tf.tile(x, [1, 1, 1, 3])\n",
    "            elif x.shape[1] == 3:  # If channels are in wrong position\n",
    "                # Transpose to move channels to end\n",
    "                return tf.transpose(x, [0, 2, 3, 1])\n",
    "            return x\n",
    "        elif len(x.shape) == 3:  # (height, width, channels)\n",
    "            if x.shape[-1] == 1:\n",
    "                return tf.tile(x, [1, 1, 3])\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {x.shape}\")\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (*input_shape[:-1], 3)\n",
    "\n",
    "\n",
    "\n",
    "# Build the Hybrid Model\n",
    "\n",
    "def build_hybrid_model(img_size=(224, 224)):\n",
    "    print(\"Building hybrid model...\")\n",
    "    \n",
    "    # Input layers with explicit shapes\n",
    "    cnn_input = layers.Input(shape=(img_size[0], img_size[1], 1), name=\"cnn_input\")\n",
    "    vit_input = layers.Input(shape=(img_size[0], img_size[1], 1), name=\"vit_input\")\n",
    "    lstm_input = layers.Input(shape=(3, img_size[0], img_size[1], 1), name=\"lstm_input\")\n",
    "    \n",
    "    # CNN Branch\n",
    "    x_cnn = TileLayer()(cnn_input)\n",
    "    efficient_net = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_size[0], img_size[1], 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    for layer in efficient_net.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    x_cnn = efficient_net(x_cnn)\n",
    "    x_cnn = layers.Dense(128, activation='relu')(x_cnn)\n",
    "    \n",
    "    # ViT Branch\n",
    "    print(\"Building ViT branch...\")\n",
    "    vit_model = build_vit(img_size=img_size)\n",
    "    # Ensure input shape matches\n",
    "    x_vit = vit_model(vit_input)\n",
    "    x_vit = layers.Dense(128, activation='relu')(x_vit)\n",
    "    \n",
    "    # BiLSTM Branch\n",
    "    print(\"Building BiLSTM branch...\")\n",
    "    # Reshape LSTM input to handle the sequence\n",
    "    x_lstm = layers.Permute((2, 3, 1, 4))(lstm_input)\n",
    "    x_lstm = layers.Reshape((img_size[0], img_size[1], 3))(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same')(x_lstm)\n",
    "    x_lstm = layers.BatchNormalization()(x_lstm)\n",
    "    x_lstm = layers.Activation('relu')(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x_lstm)\n",
    "    x_lstm = layers.BatchNormalization()(x_lstm)\n",
    "    x_lstm = layers.Activation('relu')(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.GlobalAveragePooling2D()(x_lstm)\n",
    "    x_lstm = layers.Reshape((1, -1))(x_lstm)\n",
    "    \n",
    "    x_lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x_lstm)\n",
    "    x_lstm = layers.Bidirectional(layers.LSTM(32))(x_lstm)\n",
    "    x_lstm = layers.Dense(128, activation='relu')(x_lstm)\n",
    "    x_lstm = layers.Dropout(0.3)(x_lstm)\n",
    "    \n",
    "    # 5. Feature Fusion\n",
    "    combined_features = layers.concatenate([x_cnn, x_vit, x_lstm])\n",
    "    shared_features = layers.Dense(256, activation='relu')(combined_features)\n",
    "    shared_features = layers.BatchNormalization()(shared_features)\n",
    "    shared_features = layers.Dropout(0.3)(shared_features)\n",
    "    \n",
    "    # 6. Classification Branch\n",
    "    classification_output = layers.Dense(1, activation='sigmoid', name=\"classification\")(shared_features)\n",
    "    \n",
    "    # 7. Segmentation Branch\n",
    "    print(\"Building segmentation branch...\")\n",
    "    initial_size = img_size[0] // 32\n",
    "    initial_channels = 256\n",
    "    \n",
    "    x_seg = layers.Dense(initial_size * initial_size * initial_channels)(shared_features)\n",
    "    x_seg = layers.Reshape((initial_size, initial_size, initial_channels))(x_seg)\n",
    "    \n",
    "    # Upsampling blocks\n",
    "    for filters in [128, 64, 32, 16]:\n",
    "        x_seg = layers.Conv2DTranspose(filters, 3, strides=2, padding='same')(x_seg)\n",
    "        x_seg = layers.BatchNormalization()(x_seg)\n",
    "        x_seg = layers.Activation('relu')(x_seg)\n",
    "        x_seg = layers.Conv2D(filters, 3, padding='same', activation='relu')(x_seg)\n",
    "    \n",
    "    # Final upsampling and convolution\n",
    "    x_seg = layers.Conv2DTranspose(8, 3, strides=2, padding='same')(x_seg)\n",
    "    x_seg = layers.BatchNormalization()(x_seg)\n",
    "    x_seg = layers.Activation('relu')(x_seg)\n",
    "    segmentation_output = layers.Conv2D(1, 1, activation='sigmoid', name=\"segmentation\")(x_seg)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(\n",
    "        inputs=[cnn_input, vit_input, lstm_input],\n",
    "        outputs=[classification_output, segmentation_output]\n",
    "    )\n",
    "    \n",
    "    # Enable mixed precision training\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    \n",
    "    # Compile model with updated settings\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            \"classification\": \"binary_crossentropy\",\n",
    "            \"segmentation\": \"binary_crossentropy\"\n",
    "        },\n",
    "        metrics={\n",
    "            \"classification\": [\"accuracy\", tf.keras.metrics.AUC()],\n",
    "            \"segmentation\": [\"accuracy\", tf.keras.metrics.IoU(num_classes=2, target_class_ids=[1])]\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"classification\": 1.0,\n",
    "            \"segmentation\": 0.5\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=5):\n",
    "    # Create callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"best_model.keras\",\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_classification_accuracy\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        update_freq=\"epoch\"\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, test_dataset):\n",
    "    print(\"Evaluating model...\")\n",
    "    results = model.evaluate(test_dataset, verbose=1)\n",
    "    \n",
    "    metrics = model.metrics_names\n",
    "    for i, metric in enumerate(metrics):\n",
    "        print(f\"{metric}: {results[i]:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot classification accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['classification_accuracy'])\n",
    "    plt.plot(history.history['val_classification_accuracy'])\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot classification loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['classification_loss'])\n",
    "    plt.plot(history.history['val_classification_loss'])\n",
    "    plt.title('Classification Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot segmentation accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['segmentation_accuracy'])\n",
    "    plt.plot(history.history['val_segmentation_accuracy'])\n",
    "    plt.title('Segmentation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot segmentation loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['segmentation_loss'])\n",
    "    plt.plot(history.history['val_segmentation_loss'])\n",
    "    plt.title('Segmentation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "# Prediction function\n",
    "def predict_nodules(model, image_path, img_size=(224, 224)):\n",
    "    # Load and preprocess image\n",
    "    img = load_dicom_image(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Resize and normalize\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    img_normalized = img_resized / 255.0\n",
    "    \n",
    "    # Create sequential data\n",
    "    seq_data = []\n",
    "    for i in range(3):\n",
    "        scale = 1.0 - (i * 0.1)\n",
    "        width = int(img_size[0] * scale)\n",
    "        height = int(img_size[1] * scale)\n",
    "        \n",
    "        if width < img_size[0] and height < img_size[1]:\n",
    "            x = (img_size[0] - width) // 2\n",
    "            y = (img_size[1] - height) // 2\n",
    "            \n",
    "            roi = img_normalized[y:y+height, x:x+width]\n",
    "            roi = cv2.resize(roi, img_size)\n",
    "            seq_data.append(roi)\n",
    "        else:\n",
    "            seq_data.append(img_normalized)\n",
    "    \n",
    "    seq_data = np.array(seq_data)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = {\n",
    "        \"cnn_input\": np.expand_dims(img_normalized, axis=0),\n",
    "        \"vit_input\": np.expand_dims(img_normalized, axis=0),\n",
    "        \"lstm_input\": np.expand_dims(seq_data, axis=0)\n",
    "    }\n",
    "    \n",
    "    # Make prediction\n",
    "    nodule_prob, nodule_mask = model.predict(inputs)\n",
    "    \n",
    "    # Post-process mask\n",
    "    nodule_mask = nodule_mask[0]\n",
    "    nodule_mask = (nodule_mask > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    return nodule_prob[0][0], nodule_mask\n",
    "\n",
    "# Visualize predictions\n",
    "def visualize_prediction(image_path, model, img_size=(224, 224)):\n",
    "    # Load image\n",
    "    img = load_dicom_image(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Resize image\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Make prediction\n",
    "    prob, mask = predict_nodules(model, image_path, img_size)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.title(f'Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Prediction mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.7)\n",
    "    plt.title(f'Predicted Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_resized, cmap='gray')\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.4)\n",
    "    plt.title(f'Nodule Prob: {prob:.2f}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_visualization.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting Nodule Detection Pipeline\")\n",
    "        print(\"Checking GPU availability...\")\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(f\"Found {len(gpus)} GPU(s)\")\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        else:\n",
    "            print(\"No GPU found, using CPU\")\n",
    "        \n",
    "        # Load and validate CSV data\n",
    "        if not os.path.exists(CSV_FILE):\n",
    "            print(f\"Warning: CSV file not found: {CSV_FILE}\")\n",
    "            print(\"Will attempt to create dataset without metadata\")\n",
    "        else:\n",
    "            print(f\"Loading metadata from {CSV_FILE}\")\n",
    "        \n",
    "        # Prepare dataset with proper error handling\n",
    "        dataset = prepare_dataset(max_samples=1000)\n",
    "        \n",
    "        # Set appropriate batch size based on available memory\n",
    "        if gpus:\n",
    "            batch_size = 8\n",
    "        else:\n",
    "            batch_size = 4\n",
    "        \n",
    "        img_size = (224, 224)\n",
    "        print(f\"\\nUsing batch size: {batch_size}, image size: {img_size}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset, val_dataset, test_dataset = preprocess_dataset(\n",
    "            dataset, img_size, batch_size\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        print(\"\\nBuilding model...\")\n",
    "        model = build_simple_model(img_size)\n",
    "        model.summary()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nStarting training...\")\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=10,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(\"best_model.keras\", monitor=\"val_accuracy\", \n",
    "                              save_best_only=True, mode=\"max\"),\n",
    "                EarlyStopping(monitor=\"val_accuracy\", patience=5, \n",
    "                            restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, \n",
    "                                patience=3, min_lr=1e-6)\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        results = model.evaluate(test_dataset, verbose=1)\n",
    "        print(f\"Test accuracy: {results[1]:.4f}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save model\n",
    "        model_path = 'nodule_detection_model.h5'\n",
    "        model.save(model_path)\n",
    "        print(f\"\\nModel saved as '{model_path}'\")\n",
    "        \n",
    "        # Test prediction\n",
    "        if dataset:\n",
    "            print(\"\\nTesting prediction on a sample image...\")\n",
    "            sample_image_path = dataset[0][\"image_path\"]\n",
    "            visualize_prediction(sample_image_path, model, img_size)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def build_simple_model(img_size=(224, 224)):\n",
    "    print(\"Building EfficientNet model...\")\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=(img_size[0], img_size[1], 1), name=\"image_input\")\n",
    "    \n",
    "    # Convert single channel to 3 channels\n",
    "    x = TileLayer()(inputs)\n",
    "    \n",
    "    # EfficientNet backbone\n",
    "    efficient_net = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_size[0], img_size[1], 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers for transfer learning\n",
    "    for layer in efficient_net.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = efficient_net(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"classification\")(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
