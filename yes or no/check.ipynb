{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Processed 10 files...\n",
      "Processed 20 files...\n",
      "Processed 30 files...\n",
      "Processed 40 files...\n",
      "Processed 50 files...\n",
      "Processed 60 files...\n",
      "Processed 70 files...\n",
      "Processed 80 files...\n",
      "Processed 90 files...\n",
      "Processed 100 files...\n",
      "Processed 110 files...\n",
      "Processed 120 files...\n",
      "Processed 130 files...\n",
      "Processed 140 files...\n",
      "Processed 150 files...\n",
      "Processed 160 files...\n",
      "Processed 170 files...\n",
      "Processed 180 files...\n",
      "Processed 190 files...\n",
      "Processed 200 files...\n",
      "Processed 210 files...\n",
      "Processed 220 files...\n",
      "Processed 230 files...\n",
      "Processed 240 files...\n",
      "Processed 250 files...\n",
      "Processed 260 files...\n",
      "Processed 270 files...\n",
      "Processed 280 files...\n",
      "Processed 290 files...\n",
      "Processed 300 files...\n",
      "Processed 310 files...\n",
      "Processed 320 files...\n",
      "Processed 330 files...\n",
      "Processed 340 files...\n",
      "Processed 350 files...\n",
      "Processed 360 files...\n",
      "Processed 370 files...\n",
      "Processed 380 files...\n",
      "Processed 390 files...\n",
      "Processed 400 files...\n",
      "Processed 410 files...\n",
      "Processed 420 files...\n",
      "Processed 430 files...\n",
      "Processed 440 files...\n",
      "Processed 450 files...\n",
      "Processed 460 files...\n",
      "Model saved to saved_dicom_model.pkl\n",
      "Total images processed: 463\n",
      "\n",
      "Testing image...\n",
      "Related to the dataset (Similarity: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "dataset_dir = 'C:/Users/Badari/OneDrive/Desktop/SDP/lung/dataset/images/images'\n",
    "test_image_path = 'C:/Users/Badari/OneDrive/Desktop/SDP/lung/dataset/images/images/LIDC-IDRI-0001-000001.dcm'\n",
    "model_path = 'saved_dicom_model.pkl'\n",
    "\n",
    "# Load Pre-trained Model (VGG16)\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "def process_dicom_image(image_path):\n",
    "    \"\"\"Process a DICOM file and return normalized image array\"\"\"\n",
    "    dicom_image = pydicom.dcmread(image_path)\n",
    "    pixel_array = dicom_image.pixel_array\n",
    "    \n",
    "    # Convert to 3 channels if grayscale\n",
    "    if len(pixel_array.shape) == 2:\n",
    "        pixel_array = np.stack([pixel_array] * 3, axis=-1)\n",
    "    \n",
    "    # Normalize to 0-255\n",
    "    pixel_array = ((pixel_array - np.min(pixel_array)) / \n",
    "                  (np.max(pixel_array) - np.min(pixel_array)) * 255.0)\n",
    "    return pixel_array.astype(np.uint8)\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    try:\n",
    "        # Load and process DICOM image\n",
    "        img_array = process_dicom_image(image_path)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = Image.fromarray(img_array)\n",
    "        img = img.resize((224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        \n",
    "        # Normalize and prepare for model\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Extract features with verbose=0 to reduce output\n",
    "        features = model.predict(img_array, verbose=0)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def train_and_save_model(dataset_dir, model, model_path):\n",
    "    dataset_features = []\n",
    "    processed_files = 0\n",
    "    \n",
    "    # Directly process .dcm files in the directory\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        if filename.endswith('.dcm'):\n",
    "            image_path = os.path.join(dataset_dir, filename)\n",
    "            features = extract_features(image_path, model)\n",
    "            if features is not None:\n",
    "                dataset_features.append(features)\n",
    "                processed_files += 1\n",
    "                if processed_files % 10 == 0:  # Progress update\n",
    "                    print(f\"Processed {processed_files} files...\")\n",
    "\n",
    "    if not dataset_features:\n",
    "        raise ValueError(\"No features were extracted from the dataset\")\n",
    "\n",
    "    dataset_features = np.array(dataset_features)\n",
    "    with open(model_path, 'wb') as file:\n",
    "        pickle.dump(dataset_features, file)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Total images processed: {len(dataset_features)}\")\n",
    "    return dataset_features\n",
    "\n",
    "def test_image(test_image_path, dataset_features, model, similarity_threshold=0.8):\n",
    "    if len(dataset_features) == 0:\n",
    "        return \"No features in dataset to compare against\"\n",
    "    \n",
    "    test_image_features = extract_features(test_image_path, model)\n",
    "    if test_image_features is None:\n",
    "        return \"Failed to extract features from test image\"\n",
    "    \n",
    "    similarities = cosine_similarity([test_image_features], dataset_features)\n",
    "    max_similarity = np.max(similarities)\n",
    "\n",
    "    if max_similarity > similarity_threshold:\n",
    "        return f\"Related to the dataset (Similarity: {max_similarity:.2f})\"\n",
    "    else:\n",
    "        return f\"Not related to the dataset (Similarity: {max_similarity:.2f})\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Train and Save\n",
    "        print(\"Starting training...\")\n",
    "        dataset_features = train_and_save_model(dataset_dir, vgg_model, model_path)\n",
    "\n",
    "        if len(dataset_features) > 0:\n",
    "            # Test\n",
    "            print(\"\\nTesting image...\")\n",
    "            result = test_image(test_image_path, dataset_features, vgg_model)\n",
    "            print(result)\n",
    "        else:\n",
    "            print(\"No features were extracted from the dataset\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
