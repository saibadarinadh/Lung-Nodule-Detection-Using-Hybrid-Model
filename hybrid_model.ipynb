{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Badari\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4058, Accuracy: 93.30%\n",
      "Epoch 2, Loss: 0.2013, Accuracy: 95.03%\n",
      "Epoch 3, Loss: 0.1858, Accuracy: 95.03%\n",
      "Epoch 4, Loss: 0.1221, Accuracy: 95.03%\n",
      "Epoch 5, Loss: 0.0564, Accuracy: 99.14%\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------\n",
    "# Path setup\n",
    "# -------------------------\n",
    "BASE_DIR = \"./dataset\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images/images\")\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, \"annotations/annotations/tcia-lidc-xml\")\n",
    "CSV_FILE = os.path.join(BASE_DIR, \"lidc_metadata.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata\n",
    "# -------------------------\n",
    "metadata = pd.read_csv(CSV_FILE)\n",
    "metadata['findings'] = metadata['findings'].fillna('')  # Replace NaN with empty string\n",
    "metadata['label'] = metadata['findings'].apply(lambda x: 1 if 'Nodules' in str(x) else 0)\n",
    "label_dict = dict(zip(metadata['image_id'], metadata['label']))\n",
    "\n",
    "# -------------------------\n",
    "# Get all DICOM images\n",
    "# -------------------------\n",
    "image_paths = [os.path.join(IMAGE_DIR, fname) for fname in os.listdir(IMAGE_DIR) if fname.endswith(\".dcm\")]\n",
    "\n",
    "# -------------------------\n",
    "# Custom Dataset Class\n",
    "# -------------------------\n",
    "class LIDCDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_dict, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        img_id = os.path.basename(img_path).replace(\".dcm\", \"\")\n",
    "\n",
    "        # Load DICOM Image\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image = dicom_image.pixel_array  # Extract pixel data\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        # Normalize to 0-255\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255.0\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # Get label (default: 0 if not found)\n",
    "        label = self.label_dict.get(img_id, 0)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = torch.tensor(image).permute(2, 0, 1)  # (H, W, C) â†’ (C, H, W)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Define transformations\n",
    "# -------------------------\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Create dataset and dataloader\n",
    "# -------------------------\n",
    "dataset = LIDCDataset(image_paths, label_dict, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Hybrid Model: EfficientNet + ViT + BiLSTM\n",
    "# -------------------------\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # Feature extractor - EfficientNet\n",
    "        self.efficientnet = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=0)  \n",
    "        self.efficientnet_out = self.efficientnet.num_features  \n",
    "\n",
    "        # Transformer - Vision Transformer (ViT)\n",
    "        self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.vit_out = self.vit.num_features  \n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size=self.efficientnet_out + self.vit_out, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # EfficientNet Feature Extraction\n",
    "        eff_features = self.efficientnet(x)  \n",
    "        \n",
    "        # Vision Transformer Feature Extraction\n",
    "        vit_features = self.vit(x)\n",
    "        \n",
    "        # Concatenate both feature vectors\n",
    "        combined_features = torch.cat((eff_features, vit_features), dim=1).unsqueeze(1)  \n",
    "\n",
    "        # Pass through BiLSTM\n",
    "        lstm_out, _ = self.bilstm(combined_features)\n",
    "        lstm_out = lstm_out[:, -1, :]  \n",
    "\n",
    "        # Final Classification\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# -------------------------\n",
    "# Training Setup\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridModel().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_acc = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# -------------------------\n",
    "# Save Model\n",
    "# -------------------------\n",
    "torch.save(model.state_dict(), \"hybrid_lidc_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Badari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3180, Accuracy: 91.58%\n",
      "Epoch 2, Loss: 0.1974, Accuracy: 95.03%\n",
      "Epoch 3, Loss: 0.1849, Accuracy: 95.03%\n",
      "Epoch 4, Loss: 0.0989, Accuracy: 95.03%\n",
      "Epoch 5, Loss: 0.0630, Accuracy: 98.06%\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------\n",
    "# Path setup\n",
    "# -------------------------\n",
    "BASE_DIR = \"./dataset\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images/images\")\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, \"annotations/annotations/tcia-lidc-xml\")\n",
    "CSV_FILE = os.path.join(BASE_DIR, \"lidc_metadata.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata\n",
    "# -------------------------\n",
    "metadata = pd.read_csv(CSV_FILE)\n",
    "metadata['findings'] = metadata['findings'].fillna('None')\n",
    "metadata['projection'] = metadata['projection'].fillna('Frontal')  # Default to Frontal\n",
    "metadata['label'] = metadata['findings'].apply(lambda x: 1 if 'Nodules' in str(x) else 0)\n",
    "label_dict = dict(zip(metadata['image_id'], metadata[['label', 'projection', 'findings']].values))\n",
    "\n",
    "# -------------------------\n",
    "# Get all DICOM images\n",
    "# -------------------------\n",
    "image_paths = [os.path.join(IMAGE_DIR, fname) for fname in os.listdir(IMAGE_DIR) if fname.endswith(\".dcm\")]\n",
    "\n",
    "# -------------------------\n",
    "# Custom Dataset Class\n",
    "# -------------------------\n",
    "class LIDCDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_dict, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        img_id = os.path.basename(img_path).replace(\".dcm\", \"\")\n",
    "\n",
    "        # Load DICOM Image\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image = dicom_image.pixel_array  # Extract pixel data\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        # Normalize to 0-255\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255.0\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # Get label, projection, and findings\n",
    "        label_info = self.label_dict.get(img_id, [0, 'Frontal', 'None'])\n",
    "        label, projection, findings = label_info\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = torch.tensor(image).permute(2, 0, 1)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label, img_id, projection, findings\n",
    "\n",
    "# -------------------------\n",
    "# Define transformations\n",
    "# -------------------------\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Create dataset and dataloader\n",
    "# -------------------------\n",
    "dataset = LIDCDataset(image_paths, label_dict, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Modified Model to Output Node Locations\n",
    "# -------------------------\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.efficientnet = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=0)\n",
    "        self.efficientnet_out = self.efficientnet.num_features\n",
    "        self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.vit_out = self.vit.num_features\n",
    "        self.bilstm = nn.LSTM(input_size=self.efficientnet_out + self.vit_out, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc_class = nn.Linear(512, 1)  # Binary classification\n",
    "        self.fc_bbox = nn.Linear(512, 4)  # Bounding box coordinates (x, y, w, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        eff_features = self.efficientnet(x)\n",
    "        vit_features = self.vit(x)\n",
    "        combined_features = torch.cat((eff_features, vit_features), dim=1).unsqueeze(1)\n",
    "        lstm_out, _ = self.bilstm(combined_features)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        classification = self.fc_class(lstm_out)\n",
    "        bbox = self.fc_bbox(lstm_out)\n",
    "        return classification, bbox\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridModel().to(device)\n",
    "criterion_class = nn.BCEWithLogitsLoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels, img_ids, projections, findings in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, bboxes = model(images)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss_class = criterion_class(outputs, labels)\n",
    "        loss_bbox = criterion_bbox(bboxes, torch.zeros_like(bboxes))  # Placeholder\n",
    "        loss = loss_class + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.4f}, Accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "# -------------------------\n",
    "# Save Model\n",
    "# -------------------------\n",
    "torch.save(model.state_dict(), \"hybrid_lidc_model.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
